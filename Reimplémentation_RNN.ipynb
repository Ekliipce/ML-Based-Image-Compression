{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ekliipce/Representation-and-Generative-Learning/blob/main/Reimpl%C3%A9mentation_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# RNN Reimplementation\n",
        "\n",
        "\n",
        "We aim at writing a simple RNN that will be able to sum two binary strings.\n",
        "\n",
        "Inputs will be successive bits that the network will have to add and the output is going to be the result bit.\n",
        "\n",
        "In such a case, we obviously see why a RNN might work where a simple NN could not, because of the result of the previous addition needs to be kept in memory.\n",
        "\n",
        "\\begin{matrix}\n",
        "& 1 & 0 & 1 & 1 \\\\\n",
        "+ & 0 & 0 & 0 & 1 \\\\\n",
        "= & 1 & 1 & 0 & 0 \\\\\n",
        "\\end{matrix}"
      ],
      "metadata": {
        "id": "UigHZswdz-02"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlHkdZpt1FQa"
      },
      "source": [
        "import copy\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFDgVPco1NRC"
      },
      "source": [
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Le calcul de la fonction sigmoid.\n",
        "    \"\"\"\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "def d_sigmoid(x):\n",
        "    \"\"\"\n",
        "    Le calcul de la déCopie de TPrivée de la fonction sigmoid.\n",
        "    \"\"\"\n",
        "    sig = sigmoid(x)\n",
        "    return sig * (1 - sig)\n",
        "\n",
        "\n",
        "def d_sigmoid_easy(x):\n",
        "    \"\"\"\n",
        "    Une variation de la dérivée de la sigmoid, pour simplifier certains calcul\n",
        "    lors de la rétropropagation.\n",
        "    \"\"\"\n",
        "    return x * (1 - x)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9f91RZm1VEK"
      },
      "source": [
        "def generate_dataset(d):\n",
        "    \"\"\"\n",
        "    Génère le jeu de données au format dictionnaire :\n",
        "    clef = nombre entier\n",
        "    valeur = tableau binaire correspondant (big endian)\n",
        "\n",
        "    :param d: the dimension of one bits string\n",
        "    :return: the dataset\n",
        "    \"\"\"\n",
        "    dataset = {}\n",
        "    largest_number = pow(2, d)\n",
        "    binary = np.unpackbits(np.array([range(largest_number)], dtype=np.uint8).T, axis=1)\n",
        "    for i, b in enumerate(binary):\n",
        "        dataset[i] = b\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def get_sample(dataset):\n",
        "    \"\"\"\n",
        "    Sélectionne deux nombres a et b dans l'ensemble de données et calcule la somme c.\n",
        "    Si la somme est inférieure au nombre le plus grand du dictionnaire, renvoie :\n",
        "    le tableau de bits de a, le tableau de bits de b, le tableau de bits de c\n",
        "    -> dataset[a], dataset[b], dataset[c]\n",
        "    \"\"\"\n",
        "    choices = list(dataset.keys())\n",
        "    max = choices[-1]\n",
        "\n",
        "    while(True):\n",
        "      a = int(np.random.choice(choices))\n",
        "      b = int(np.random.choice(choices))\n",
        "      c = a + b\n",
        "      if (c < max):\n",
        "        bit_a = dataset[a]\n",
        "        bit_b = dataset[b]\n",
        "        bit_c = dataset[c]\n",
        "\n",
        "        return bit_a, bit_b, bit_c"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = generate_dataset(10)\n",
        "get_sample(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEs6atuSC94Q",
        "outputId": "272dbdee-e96d-4d75-80bb-1c3cbfe6ffd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0, 1, 1, 1, 1, 0, 1, 1], dtype=uint8),\n",
              " array([1, 1, 0, 1, 1, 1, 0, 1], dtype=uint8),\n",
              " array([0, 1, 0, 1, 1, 0, 0, 0], dtype=uint8))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-v4NoWi1Z95"
      },
      "source": [
        "def init_nn(inp_dim, hid_dim, out_dim):\n",
        "    \"\"\"\n",
        "    Initialise les paramètres du réseaux : wi, wh, wo ainsi que les tenseurs\n",
        "    des mêmes tailles qui serviront pour les mettre à jour.\n",
        "\n",
        "    wi, wh, wo sont initialisés aléatoirement suivant une loi uniforme allant\n",
        "    de -1 à 1.\n",
        "\n",
        "    Utiliser la fonction numpy.random.random.\n",
        "\n",
        "    wi_update, wh_update, wo_update ont la même taille que wi, wh et wo\n",
        "    respectivement mais ne contiennent à l'initialisation que des 0.\n",
        "\n",
        "    Utiliser soit :\n",
        "    - np.zeros -> prend une taille en argument\n",
        "    - np.zeros_like -> prend un tenseur en argument\n",
        "\n",
        "    :param inp_dim: dimension de l'entrée\n",
        "    :param hid_dim: dimension de la couche cachée\n",
        "    :param out_dim: dimension de la sortie\n",
        "    :return: les paramètres\n",
        "    \"\"\"\n",
        "    wi_layer = np.random.random((hid_dim, inp_dim))\n",
        "    wh_layer = np.random.random((hid_dim, hid_dim))\n",
        "    wo_layer = np.random.random((out_dim, hid_dim))\n",
        "    return wi_layer, wh_layer, wo_layer"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Forward\n",
        "$$a_t = w_ix_t + w_hh_{t-1}$$\n",
        "\n",
        "$$h_t = σ(a_t)$$\n",
        "\n",
        "$$o_t = w_oh_t$$\n",
        "\n",
        "$$ŷ = \\sigma(o_t)$$\n",
        "\n",
        "\n",
        "\n",
        "## Backward\n",
        "With $L$, loss function as $\\sum^N_{i=0}(ŷ - y)²$ : <br/><br/>\n",
        "\n",
        "1. $$\\frac{∂L}{\\partial w_o} = \\frac{∂L_t}{\\partial o_t} \\frac{∂o_t}{\\partial w_o} = \\frac{∂L_t}{\\partial o_t} \\frac{∂w_oh_t}{\\partial w_o} = \\frac{∂L_t}{\\partial o_t} h_t$$\n",
        "and $$\\frac{∂L}{\\partial o_t} = \\frac{∂L_t}{\\partial ŷ_t}\\frac{∂ŷ_t}{\\partial o_t} = \\frac{∂(ŷ - y)²}{\\partial ŷ_t}\\frac{∂σ(o_t)}{\\partial o_t} = 2(ŷ-y)*ŷ(1-ŷ)$$\n",
        "so $$\\frac{∂L}{\\partial w_o} = 2(ŷ-y)*ŷ(1-ŷ) * h_t$$\n",
        "\n",
        "___\n",
        "<br/><br/>\n",
        "\n",
        "2. $$\\frac{\\delta L}{\\delta w_i} = \\frac{\\delta L}{\\delta h_t}\\frac{\\delta h_t}{\\delta a_t}\\frac{\\delta a_t}{\\delta w_i} = \\frac{\\delta L}{\\delta h_t}h_t(1-h_t)x_t$$\n",
        "\n",
        "<br/><br/>\n",
        "___\n",
        "\n",
        "<br/><br/>\n",
        "\n",
        "3. $$\\frac{\\delta L}{\\delta w_h} = \\frac{\\delta L}{\\delta h_t} . \\frac{\\delta h_t}{\\delta a_t}.\\frac{\\delta a_t}{\\delta w_h} = h_{t-1}h_t(1 - h_t)\\frac{\\delta L}{\\delta h_t}$$\n",
        "\n",
        "<br/><br/>\n",
        "\n",
        "___\n",
        "\n",
        "with\n",
        "\n",
        "$$\\frac{\\delta L}{\\delta h_t} = \\frac{\\delta L_t}{\\delta h_t} + \\frac{\\delta L_{i > t}}{\\delta h_t} = \\frac{\\delta L_t}{\\delta o_t}.\\frac{\\delta o_t}{\\delta h_t} + \\frac{\\delta L}{\\delta h_{t+1}}.\\frac{\\delta h_{t+1}}{\\delta a_{t+1}}.\\frac{\\delta a_{t+1}}{\\delta h_t}$$\n",
        "\n",
        "$$\\frac{\\delta L}{\\delta h_t} = w_o\\frac{\\delta L_t}{\\delta o_t} + w_h h_{t+1}(1 - h_{t+1})\\frac{\\delta L} {\\delta h_{t+1}}$$\n"
      ],
      "metadata": {
        "id": "wGRTrVM_9OBu"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9wZOlJa1dDO"
      },
      "source": [
        "def train(wi, wh, wo, iterations, dataset):\n",
        "    for i in range(iterations):\n",
        "        a, b, c = get_sample(dataset)\n",
        "        d = np.zeros_like(c)  # stores our successive predictions\n",
        "        error = 0\n",
        "        inputs = []\n",
        "        truth = []\n",
        "        o_deltas = []  # stores gradient of error relative to ot\n",
        "        ht_values = []  # stores values of hidden states\n",
        "        ht_values.append(np.zeros((hidden_dimension, 1)))\n",
        "\n",
        "        # Forward\n",
        "        for pos in range(number_bits):\n",
        "            \"\"\"\n",
        "            1. On récupère les premiers bits de a et de b -> devient notre x\n",
        "            2. On récupère le premier bit de c -> devient notre y\n",
        "\n",
        "            \"\"\"\n",
        "            input = np.array([a[pos], b[pos]]).reshape(-1, 1)\n",
        "            y = np.array([c[pos]])\n",
        "\n",
        "            inputs.append(input)\n",
        "            truth.append(y)\n",
        "\n",
        "            # Compute a_t\n",
        "            a_t = wi @ input + wh @ ht_values[pos]\n",
        "\n",
        "            # Compute h_t\n",
        "            h_t = sigmoid(a_t)\n",
        "            ht_values.append(h_t)\n",
        "\n",
        "            # Compute o_t\n",
        "            o_t = wo @ h_t\n",
        "\n",
        "            # Compute y_pred\n",
        "            y_pred = sigmoid(o_t)\n",
        "\n",
        "            d[pos] = np.around(y_pred)\n",
        "\n",
        "\n",
        "            o_delta = (y_pred - y) * d_sigmoid(y_pred)\n",
        "            o_deltas.append(o_delta)\n",
        "\n",
        "\n",
        "        future_ht_delta = np.zeros_like(ht_values[-1])  # stores delta L / delta ht+1, necessary when computing delta L / delta ht+1\n",
        "        future_ht = np.zeros_like(ht_values[-1])  # stores ht+1, 0 when we are at last t\n",
        "\n",
        "        # Backward\n",
        "\n",
        "        wi_update = np.zeros_like(wi)\n",
        "        wh_update = np.zeros_like(wh)\n",
        "        wo_update = np.zeros_like(wo)\n",
        "\n",
        "\n",
        "        for pos in reversed(range(number_bits)):\n",
        "            \"\"\"\n",
        "            1. On récupère le x en partant de la fin de la séquence (le premier bit)\n",
        "            2. On récupère la valeur de son état caché ht dans la liste\n",
        "            3. On récupère la valeur de l'état caché précédent dans la liste ht-1\n",
        "            4. On récupère la valeur de delta L / delta ot dans la liste\n",
        "            5. On calcule ht_delta (delta L / delta ht) -> partie la plus complexe\n",
        "            6. On calcule wo_update, wh_update, wi_update (gradients pour wo, wh, wi)\n",
        "            \"\"\"\n",
        "            # Get first bit\n",
        "            x_i = inputs[pos]\n",
        "            y_pred = d[pos]\n",
        "            y = truth[pos]\n",
        "\n",
        "            # Get h_t\n",
        "            h_t = ht_values[pos + 1]\n",
        "\n",
        "\n",
        "            # Get h_{t-1}\n",
        "            h_t_prev = ht_values[pos]\n",
        "\n",
        "            # Get partial derivative L with respect to o_t\n",
        "            o_delta = o_deltas[pos]\n",
        "\n",
        "            # Get partial derivative L with respect to h_t\n",
        "            h_delta = (wo.T @ o_delta + future_ht_delta) * d_sigmoid(h_t)\n",
        "\n",
        "            # Compute partials derivative L with respect to weights\n",
        "            wo_update += np.outer(o_delta, h_t)\n",
        "            wh_update += np.outer(h_delta, h_t_prev)\n",
        "            wi_update += np.outer(h_delta, x_i)\n",
        "\n",
        "            # Update values\n",
        "            h_delta_next = h_delta\n",
        "            h_t_next = h_t\n",
        "\n",
        "        wi -= 0.001 * wi_update\n",
        "        wh -= 0.001 * wh_update\n",
        "        wo -= 0.001 * wo_update\n",
        "\n",
        "        if i % 1000 == 0:\n",
        "            print(f\"[{i}|{iterations}] Error: {error}\")\n",
        "            print(f\"\\tTrue: {a} + {b} = {c}\")\n",
        "            print(f\"\\tPred: {d}\")"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOcPbIp-1ewF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da74136f-418e-4c80-d854-36e87c0c56a1"
      },
      "source": [
        "alpha = 0.1\n",
        "input_dimension = 2\n",
        "hidden_dimension = 16\n",
        "output_dimension = 1\n",
        "number_bits = 8\n",
        "data = generate_dataset(number_bits)\n",
        "nn = init_nn(input_dimension, hidden_dimension, output_dimension)\n",
        "train(*nn, 10000, data)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0|10000] Error: 0\n",
            "\tTrue: [0 1 0 0 1 1 1 1] + [0 1 0 1 1 0 1 0] = [1 0 1 0 1 0 0 1]\n",
            "\tPred: [1 1 1 1 1 1 1 1]\n",
            "[1000|10000] Error: 0\n",
            "\tTrue: [0 0 0 1 0 0 0 0] + [1 1 0 0 0 1 0 1] = [1 1 0 1 0 1 0 1]\n",
            "\tPred: [1 1 1 1 1 1 1 1]\n",
            "[2000|10000] Error: 0\n",
            "\tTrue: [0 0 0 0 0 1 1 1] + [0 1 0 0 1 1 0 1] = [0 1 0 1 0 1 0 0]\n",
            "\tPred: [1 1 1 1 1 1 1 1]\n",
            "[3000|10000] Error: 0\n",
            "\tTrue: [0 0 0 0 0 1 0 0] + [0 0 1 0 0 0 0 0] = [0 0 1 0 0 1 0 0]\n",
            "\tPred: [1 1 1 1 1 1 1 1]\n",
            "[4000|10000] Error: 0\n",
            "\tTrue: [0 1 0 1 1 0 1 0] + [0 0 0 1 0 1 1 1] = [0 1 1 1 0 0 0 1]\n",
            "\tPred: [1 1 1 1 1 1 1 1]\n",
            "[5000|10000] Error: 0\n",
            "\tTrue: [1 0 1 0 1 0 1 0] + [0 0 0 1 0 0 1 0] = [1 0 1 1 1 1 0 0]\n",
            "\tPred: [1 1 1 1 1 1 1 1]\n",
            "[6000|10000] Error: 0\n",
            "\tTrue: [1 1 0 1 1 1 0 1] + [0 0 0 0 0 0 0 0] = [1 1 0 1 1 1 0 1]\n",
            "\tPred: [1 1 1 1 1 1 1 1]\n",
            "[7000|10000] Error: 0\n",
            "\tTrue: [0 0 1 1 1 1 1 1] + [1 0 1 0 1 1 1 0] = [1 1 1 0 1 1 0 1]\n",
            "\tPred: [1 1 1 1 1 1 1 1]\n",
            "[8000|10000] Error: 0\n",
            "\tTrue: [0 0 0 1 0 0 1 1] + [1 1 0 1 0 1 1 1] = [1 1 1 0 1 0 1 0]\n",
            "\tPred: [1 1 1 1 1 1 1 1]\n",
            "[9000|10000] Error: 0\n",
            "\tTrue: [0 0 0 1 0 0 1 0] + [1 1 0 0 0 1 1 1] = [1 1 0 1 1 0 0 1]\n",
            "\tPred: [1 1 1 1 1 1 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwG2MaNh10la"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}